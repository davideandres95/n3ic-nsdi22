{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add license"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import larq as lq\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.python.keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_list = pd.read_csv('datasets/UNSW_IOT/List_Of_Devices.txt', sep='\\t+')\n",
    "data_df = pd.read_csv('datasets/UNSW_IOT/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add DeviceName column from List_Of_Devices.txt\n",
    "\n",
    "device_list['MAC ADDRESS'] = device_list['MAC ADDRESS'].str.strip()\n",
    "macs = set(data_df.SrcMac.unique()).union( set(data_df.DstMac.unique()) )\n",
    "\n",
    "mac_to_device = {}\n",
    "for m in macs:\n",
    "    dev_name = device_list[device_list['MAC ADDRESS'] == m]['List of Devices'].values\n",
    "    if len(dev_name) == 0:\n",
    "        mac_to_device[m] = None\n",
    "    else:\n",
    "        mac_to_device[m] = dev_name[0]\n",
    "\n",
    "def mac_label(x, labels):\n",
    "    if x['SrcMac'] == 'ff:ff:ff:ff:ff:ff':\n",
    "        label_mac = x['DstMac']\n",
    "    elif x['DstMac'] == 'ff:ff:ff:ff:ff:ff':\n",
    "        label_mac = x['SrcMac']\n",
    "    elif x['SrcMac'] == '14:cc:20:51:33:ea':\n",
    "        label_mac = x['DstMac']\n",
    "    elif x['DstMac'] == '14:cc:20:51:33:ea':\n",
    "        label_mac = x['SrcMac']\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    return labels[label_mac]\n",
    "\n",
    "data_df['DeviceName'] = data_df.apply(lambda x: mac_label(x, mac_to_device), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data pre-processing\n",
    "\n",
    "columns_to_drop = ['State', 'Dir', 'SrcMac', 'DstMac', 'SrcAddr', 'DstAddr',\n",
    "                   'StartTime', 'TotPkts', 'TotBytes', 'Flgs', 'Sport', 'Dport']\n",
    "data_df = data_df.drop(columns_to_drop, axis=1)\n",
    "\n",
    "cols = ['Proto']\n",
    "for col in cols:\n",
    "    data_df[col] =  data_df[col].astype('category')\n",
    "    data_df[col] =  data_df[col].cat.codes\n",
    "\n",
    "# Removing rows with NaN in any column\n",
    "data_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLES_PER_CLS_THRESHOLD = 40000\n",
    "\n",
    "to_classify = []\n",
    "for item in data_df.DeviceName.value_counts().items():\n",
    "    if (item[1] > SAMPLES_PER_CLS_THRESHOLD and not (item[0] in ['Laptop', 'Samsung Galaxy Tab', 'MacBook'])):\n",
    "        print(item)\n",
    "        to_classify.append(item[0])\n",
    "len(to_classify)\n",
    "\n",
    "assert len(to_classify) == 9, 'Tune SAMPLES_PER_CLS_THRESHOLD to obtain 9 classes (got %d)' % len(to_classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning the group labels, keeping at most 43k samples per class\n",
    "\n",
    "label_array = np.zeros(data_df.shape[0])\n",
    "data_df['label'] = label_array.astype(int)\n",
    "\n",
    "for i, name in enumerate(to_classify):\n",
    "    index = data_df[data_df.DeviceName == name ].index\n",
    "    data_df.loc[index,'label'] = int(i+1)\n",
    "\n",
    "# Shuffling data around to ensure we do not select devices of a single time from the \"rest\" catory\n",
    "data_df = data_df.sample(frac=1, random_state=0)\n",
    "\n",
    "data_df = data_df.groupby(data_df.label).head(43000)\n",
    "\n",
    "# Dropping non numerical columns\n",
    "data_df = data_df.drop('DeviceName', axis=1)\n",
    "\n",
    "data_df = data_df.apply(pd.to_numeric, errors='raise')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "    'proto',\n",
    "    'dur',\n",
    "    'sbytes', 'dbytes',\n",
    "    'sttl', 'dttl',\n",
    "    'sload', 'dload',\n",
    "    'spkts', 'dpkts',\n",
    "    'smean', 'dmean',\n",
    "    'sinpkt', 'dinpkt',\n",
    "    'tcprtt', 'synack', 'ackdat',\n",
    "    'label'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['State', 'Dir', 'SrcMac', 'DstMac', 'SrcAddr', 'DstAddr',\n",
    "                   'StartTime', 'TotPkts', 'TotBytes', 'Flgs', 'Sport', 'Dport']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_df[data_df.columns[:17]].values\n",
    "Y = data_df[data_df.columns[17]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data binarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xint = X.astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_in_bits = {\n",
    "    'proto': 8,\n",
    "    'dur': 16,\n",
    "    'sbytes': 24, 'dbytes': 24,\n",
    "    'sttl': 8, 'dttl': 8,\n",
    "    'sload': 24, 'dload': 24,\n",
    "    'spkts': 16, 'dpkts': 16,\n",
    "    'smean': 16, 'dmean': 16,\n",
    "    'sinpkt': 16, 'dinpkt': 16,\n",
    "    'tcprtt': 8, 'synack': 8, 'ackdat': 8,\n",
    "}\n",
    "\n",
    "sum(size_in_bits.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xbin = np.zeros( (Xint.shape[0], sum(size_in_bits.values())) )\n",
    "for i, feature_row in enumerate(Xint):\n",
    "    # the index at which the next binary value should be written\n",
    "    write_ptr = 0\n",
    "    for j, column_val in enumerate(feature_row):\n",
    "        # Transforming in KB sbytes, dbytes, sload, dload\n",
    "        if j in [2,3,6,7]:\n",
    "            column_val = int(column_val/1000) \n",
    "        # Setting to maximum any value above the max given the number of b\n",
    "        if (column_val > 2**size_in_bits[selected_columns[j]] - 1):\n",
    "            column_val = 2**size_in_bits[selected_columns[j]] - 1\n",
    "        tmp = list(bin(column_val)[2:])\n",
    "        tmp = [int(x) for x in tmp]\n",
    "        # zero padding to the left\n",
    "        tmp = [0]*(size_in_bits[selected_columns[j]] - len(tmp)) + tmp\n",
    "        for k, bin_val in enumerate(tmp):\n",
    "            Xbin[i,write_ptr] = bin_val\n",
    "            write_ptr += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BNN dataset\n",
    "Xbin[Xbin == 0] = -1\n",
    "X_bin = Xbin\n",
    "\n",
    "Y_cat = np_utils.to_categorical(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix_wo_clf(y_true, y_pred, class_names, normalize='true'):\n",
    "    cm = confusion_matrix(y_true, y_pred, normalize=normalize)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    if normalize == 'true':\n",
    "        disp.plot(cmap=plt.cm.Blues, ax=ax, values_format='.2f')\n",
    "    else:\n",
    "        disp.plot(cmap=plt.cm.Blues, ax=ax)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_multiclass_dataset(clf, X_test, y_test, y_pred, y_score, is_bnn=False):\n",
    "    \n",
    "    if is_bnn:\n",
    "        # Make y_test 1D\n",
    "        y_test = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    cm_data = plot_confusion_matrix_wo_clf(y_test, y_pred, map(str,range(max(y_test)+1)))\n",
    "    \n",
    "    mcf = multilabel_confusion_matrix(y_test, y_pred)\n",
    "    fpr_list = []\n",
    "    fnr_list = []\n",
    "    tpr_list = []\n",
    "    for cf in mcf:\n",
    "        tn, fp, fn, tp = cf.ravel()\n",
    "        fpr_list.append(fp / (fp+tn))\n",
    "        fnr_list.append(fn / (fn+tp))\n",
    "        tpr_list.append(tp / (tp+fn))\n",
    "            \n",
    "    a = accuracy_score(y_test, y_pred)\n",
    "    p = precision_score(y_test, y_pred, average='macro')\n",
    "    r = recall_score(y_test, y_pred, average='macro')\n",
    "    tpr_ = r # TPR = Recall\n",
    "    assert np.average(tpr_list) == tpr_\n",
    "    fpr_ = np.average(fpr_list)\n",
    "    fnr_ = np.average(fnr_list)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    y_score = to_categorical(y_pred)\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    roc_auc = {}\n",
    "    num_classes = clf.output.shape[1] if isinstance(clf, tf.keras.models.Sequential) else len(clf.classes_)\n",
    "    for i in range(num_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(to_categorical(y_test)[:, i], y_score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    ra = roc_auc_score(y_test, y_score, average='macro', multi_class='ovr')\n",
    "\n",
    "    return a, p, r, tpr_, fpr_, fnr_, f1, ra, cm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bnn_model(neurons, \n",
    "                    input_shape, \n",
    "                    last_act=\"softmax\", \n",
    "                    learning_rate=0.0001, \n",
    "                    loss='squared_hinge'):\n",
    "    \n",
    "    kwargs = dict(input_quantizer=\"ste_sign\",\n",
    "              kernel_quantizer=\"ste_sign\",\n",
    "              kernel_constraint=\"weight_clip\")\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(lq.layers.QuantDense(neurons[0], use_bias=False,\n",
    "                                   input_quantizer=\"ste_sign\",\n",
    "                                   kernel_quantizer=\"ste_sign\",\n",
    "                                   kernel_constraint=\"weight_clip\",\n",
    "                                   input_shape=(input_shape,) ) )\n",
    "    model.add(tf.keras.layers.BatchNormalization(scale=False, momentum=0.9))\n",
    "    model.add(lq.layers.QuantDense(neurons[1], use_bias=False, **kwargs))\n",
    "    model.add(tf.keras.layers.BatchNormalization(scale=False, momentum=0.9))\n",
    "    model.add(lq.layers.QuantDense(neurons[2], use_bias=False, activation=last_act, **kwargs))\n",
    "\n",
    "    # lq.models.summary(model)\n",
    "    \n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=opt, loss=loss, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths_range = [3, 6, 9]\n",
    "estimators_range = [5]\n",
    "\n",
    "bnn_models = [\n",
    "    [32, 16, 10],\n",
    "    [64, 32, 10],\n",
    "    [128, 64, 10]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_folds = 5\n",
    "train_epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_store_iot = {}\n",
    "precision_store_iot = {}\n",
    "recall_store_iot = {}\n",
    "tpr_store_iot = {}\n",
    "fpr_store_iot = {}\n",
    "fnr_store_iot = {}\n",
    "f1_store_iot = {}\n",
    "roc_auc_store_iot = {}\n",
    "cm_data_store_iot = {}\n",
    "\n",
    "########################################\n",
    "\n",
    "skf = StratifiedKFold(n_splits=num_folds)\n",
    "\n",
    "for depth in depths_range:\n",
    "    label = 'dt__depth_%d' % (depth)\n",
    "    accuracy_store_iot[label] = np.zeros(num_folds)\n",
    "    precision_store_iot[label] = np.zeros(num_folds)\n",
    "    recall_store_iot[label] = np.zeros(num_folds)\n",
    "    tpr_store_iot[label] = np.zeros(num_folds)\n",
    "    fpr_store_iot[label] = np.zeros(num_folds)\n",
    "    fnr_store_iot[label] = np.zeros(num_folds)\n",
    "    f1_store_iot[label] = np.zeros(num_folds)\n",
    "    roc_auc_store_iot[label] = np.zeros(num_folds)\n",
    "    cm_data_store_iot[label] = {}\n",
    "    \n",
    "    fold_idx = 0\n",
    "    for train_index, test_index in skf.split(X, Y):\n",
    "        print('DT: depth=%d, fold_idx=%d' % (depth, fold_idx))\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = Y[train_index], Y[test_index]\n",
    "        \n",
    "        dt = DecisionTreeClassifier(criterion='entropy', max_depth=depth, random_state=0)\n",
    "        dt = dt.fit(X_train, y_train)\n",
    "        y_pred = dt.predict(X_test)\n",
    "        y_score = dt.predict_proba(X_test)\n",
    "        \n",
    "        a, p, r, tpr, fpr, fnr, f1, roc_auc, cm_data = metrics_multiclass_dataset(dt, X_test, y_test, y_pred, y_score)\n",
    "        accuracy_store_iot[label][fold_idx] = a\n",
    "        precision_store_iot[label][fold_idx] = p\n",
    "        recall_store_iot[label][fold_idx] = r\n",
    "        tpr_store_iot[label][fold_idx] = tpr\n",
    "        fpr_store_iot[label][fold_idx] = fpr\n",
    "        fnr_store_iot[label][fold_idx] = fnr\n",
    "        f1_store_iot[label][fold_idx] = f1\n",
    "        roc_auc_store_iot[label][fold_idx] = roc_auc\n",
    "        cm_data_store_iot[label][fold_idx] = cm_data\n",
    "        \n",
    "        fold_idx += 1\n",
    "\n",
    "        print('-'*40)\n",
    "    print('='*80)\n",
    "\n",
    "########################################################################################\n",
    "\n",
    "for depth in depths_range:\n",
    "    for estimators in estimators_range:\n",
    "        label = 'rf__depth_%d__estimators_%d' % (depth, estimators)\n",
    "        accuracy_store_iot[label] = np.zeros(num_folds)\n",
    "        precision_store_iot[label] = np.zeros(num_folds)\n",
    "        recall_store_iot[label] = np.zeros(num_folds)\n",
    "        tpr_store_iot[label] = np.zeros(num_folds)\n",
    "        fpr_store_iot[label] = np.zeros(num_folds)\n",
    "        fnr_store_iot[label] = np.zeros(num_folds)\n",
    "        f1_store_iot[label] = np.zeros(num_folds)\n",
    "        roc_auc_store_iot[label] = np.zeros(num_folds)\n",
    "        cm_data_store_iot[label] = {}\n",
    "\n",
    "        fold_idx = 0\n",
    "        for train_index, test_index in skf.split(X, Y):\n",
    "            print('RF: depth=%d, estimators=%d, fold_idx=%d' % (depth, estimators, fold_idx))\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "            rf = RandomForestClassifier(criterion='entropy', max_depth=depth, n_estimators=estimators, random_state=0)\n",
    "            rf = rf.fit(X_train, y_train)\n",
    "            y_pred = rf.predict(X_test)\n",
    "            y_score = rf.predict_proba(X_test)\n",
    "\n",
    "            a, p, r, tpr, fpr, fnr, f1, roc_auc, cm_data = metrics_multiclass_dataset(rf, X_test, y_test, y_pred, y_score)\n",
    "            accuracy_store_iot[label][fold_idx] = a\n",
    "            precision_store_iot[label][fold_idx] = p\n",
    "            recall_store_iot[label][fold_idx] = r\n",
    "            tpr_store_iot[label][fold_idx] = tpr\n",
    "            fpr_store_iot[label][fold_idx] = fpr\n",
    "            fnr_store_iot[label][fold_idx] = fnr\n",
    "            f1_store_iot[label][fold_idx] = f1\n",
    "            roc_auc_store_iot[label][fold_idx] = roc_auc\n",
    "            cm_data_store_iot[label][fold_idx] = cm_data\n",
    "        \n",
    "            fold_idx += 1\n",
    "\n",
    "        print('-'*40)\n",
    "    print('='*80)\n",
    "\n",
    "########################################################################################\n",
    "\n",
    "for neurons in bnn_models:\n",
    "    label = 'bnn__%s' % ('_'.join(map(str, neurons)))\n",
    "    accuracy_store_iot[label] = np.zeros(num_folds)\n",
    "    precision_store_iot[label] = np.zeros(num_folds)\n",
    "    recall_store_iot[label] = np.zeros(num_folds)\n",
    "    tpr_store_iot[label] = np.zeros(num_folds)\n",
    "    fpr_store_iot[label] = np.zeros(num_folds)\n",
    "    fnr_store_iot[label] = np.zeros(num_folds)\n",
    "    f1_store_iot[label] = np.zeros(num_folds)\n",
    "    roc_auc_store_iot[label] = np.zeros(num_folds)\n",
    "    cm_data_store_iot[label] = {}\n",
    "    \n",
    "    fold_idx = 0\n",
    "    for train_index, test_index in skf.split(X, Y):\n",
    "        print('BNN', neurons ,', fold_idx=%d' % (fold_idx))\n",
    "        X_train, X_test = X_bin[train_index], X_bin[test_index]\n",
    "        y_train, y_test = Y_cat[train_index], Y_cat[test_index]\n",
    "        \n",
    "        model = build_bnn_model(neurons, X_bin.shape[1])   \n",
    "        fname = 'bnn__iot__%s__fold%d.h5' % ('_'.join(map(str, neurons)), fold_idx)\n",
    "        \n",
    "        model_checkpoint_callback = ModelCheckpoint(\n",
    "            filepath='models/' + fname,\n",
    "            monitor='val_accuracy',\n",
    "            mode='max',\n",
    "            save_best_only=True,\n",
    "            verbose=1)\n",
    "        \n",
    "        if not os.path.isfile('models/' + fname):\n",
    "            train_history = model.fit(X_train, y_train, \n",
    "                              batch_size=batch_size, \n",
    "                              epochs=train_epochs,\n",
    "                              verbose=0,\n",
    "                              validation_data=(X_test, y_test),\n",
    "                              callbacks=[model_checkpoint_callback])\n",
    "            \n",
    "            # Reload best weights\n",
    "            model.load_weights('models/' + fname)\n",
    "        else:\n",
    "            # Reload stored weights\n",
    "            print('Loading models/' + fname)\n",
    "            model.load_weights('models/' + fname)\n",
    "\n",
    "        y_pred = model.predict_classes(X_test)\n",
    "        y_score = model.predict_proba(X_test)\n",
    "        \n",
    "        a, p, r, tpr, fpr, fnr, f1, roc_auc, cm_data = metrics_multiclass_dataset(model, X_test, y_test, y_pred, y_score, is_bnn=True)\n",
    "        accuracy_store_iot[label][fold_idx] = a\n",
    "        precision_store_iot[label][fold_idx] = p\n",
    "        recall_store_iot[label][fold_idx] = r\n",
    "        tpr_store_iot[label][fold_idx] = tpr\n",
    "        fpr_store_iot[label][fold_idx] = fpr\n",
    "        fnr_store_iot[label][fold_idx] = fnr\n",
    "        f1_store_iot[label][fold_idx] = f1\n",
    "        roc_auc_store_iot[label][fold_idx] = roc_auc\n",
    "        cm_data_store_iot[label][fold_idx] = cm_data\n",
    "        \n",
    "        fold_idx += 1\n",
    "\n",
    "        print('-'*40)\n",
    "    print('='*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for store,metric in zip([accuracy_store_iot, precision_store_iot, recall_store_iot,\n",
    "                         fnr_store_iot, fpr_store_iot, f1_store_iot, roc_auc_store_iot],\n",
    "                        ['Accuracy', 'Precision', 'Recall', 'FNR', 'FPR', 'F1-score', 'ROC-AUC']):\n",
    "    print('[%s]' % metric)\n",
    "    for key in store:\n",
    "        print('%s: %.1f ± %.1f' % (key, 100*store[key].mean(), 100*store[key].std()))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
