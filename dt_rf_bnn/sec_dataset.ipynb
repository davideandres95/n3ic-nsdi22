{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add license"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import larq as lq\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.python.keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('datasets/UNSW_NB15/UNSW_NB15_training-set.csv', delimiter=',')\n",
    "test_df = pd.read_csv('datasets/UNSW_NB15/UNSW_NB15_testing-set.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = train_df.append(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df.drop(['id'], axis=1)\n",
    "cols = ['proto', 'service', 'state']\n",
    "for col in cols:\n",
    "    data_df[col] =  data_df[col].astype('category')\n",
    "    data_df[col] =  data_df[col].cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "    'dur',\n",
    "    'proto',\n",
    "    'sbytes', 'dbytes',\n",
    "    'sttl', 'dttl',\n",
    "    'sload', 'dload',\n",
    "    'spkts', 'dpkts',\n",
    "    'smean', 'dmean',\n",
    "    'sinpkt', 'dinpkt',\n",
    "    'tcprtt', 'synack', 'ackdat',\n",
    "    'ct_src_ltm', 'ct_dst_ltm',\n",
    "    'ct_dst_src_ltm',\n",
    "\n",
    "    'attack_cat'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df[selected_columns]\n",
    "X = data_df[data_df.columns[:20]].values\n",
    "Y = data_df[data_df.columns[20]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data binarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xint = X.astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_in_bits = {\n",
    "    'dur': 8,\n",
    "    'proto': 8,\n",
    "    'sbytes': 16, 'dbytes': 16,\n",
    "    'sttl': 8, 'dttl': 8,\n",
    "    'sload': 24, 'dload': 24,\n",
    "    'spkts': 16, 'dpkts': 16,\n",
    "    'smean': 16, 'dmean': 16,\n",
    "    'sinpkt': 16, 'dinpkt': 16,\n",
    "    'tcprtt': 8, 'synack': 8, 'ackdat': 8,\n",
    "    'ct_src_ltm': 8, 'ct_dst_ltm': 8,\n",
    "    'ct_dst_src_ltm': 8,\n",
    "}\n",
    "\n",
    "sum(size_in_bits.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xbin = np.zeros( (Xint.shape[0], sum(size_in_bits.values())) )\n",
    "for i, feature_row in enumerate(Xint):\n",
    "    # the index at which the next binary value should be written\n",
    "    write_ptr = 0\n",
    "    for j, column_val in enumerate(feature_row):\n",
    "        # Transforming in KB sbytes, dbytes, sload, dload\n",
    "        if j in [2,3,6,7]:\n",
    "            column_val = int(column_val/1000) \n",
    "        # Setting to maximum any value above the max given the number of b\n",
    "        if (column_val > 2**size_in_bits[selected_columns[j]] - 1):\n",
    "            column_val = 2**size_in_bits[selected_columns[j]] - 1\n",
    "        tmp = list(bin(column_val)[2:])\n",
    "        tmp = [int(x) for x in tmp]\n",
    "        # zero padding to the left\n",
    "        tmp = [0]*(size_in_bits[selected_columns[j]] - len(tmp)) + tmp\n",
    "        for k, bin_val in enumerate(tmp):\n",
    "            Xbin[i,write_ptr] = bin_val\n",
    "            write_ptr += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DT/RF dataset\n",
    "Y__2_classes = Y.copy()\n",
    "Y__2_classes[Y == 'Normal'] = 0\n",
    "Y__2_classes[Y != 'Normal'] = 1\n",
    "Y__2_classes = Y__2_classes.astype('int')\n",
    "\n",
    "# BNN dataset\n",
    "Xbin[Xbin == 0] = -1\n",
    "X_bin = Xbin\n",
    "\n",
    "Y_cat__2_classes = np_utils.to_categorical(Y__2_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix_wo_clf(y_true, y_pred, class_names, normalize='true'):\n",
    "    cm = confusion_matrix(y_true, y_pred, normalize=normalize)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    if normalize == 'true':\n",
    "        disp.plot(cmap=plt.cm.Blues, ax=ax, values_format='.2f')\n",
    "    else:\n",
    "        disp.plot(cmap=plt.cm.Blues, ax=ax)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_binary_dataset(clf, X_test, y_test, y_pred, y_score, is_bnn=False):\n",
    "    \n",
    "    if is_bnn:\n",
    "        # Make y_test 1D\n",
    "        y_test = np.argmax(y_test, axis=1)\n",
    "        \n",
    "    plot_confusion_matrix_wo_clf(y_test, y_pred, ['0','1'])\n",
    "   \n",
    "    a = accuracy_score(y_test, y_pred)\n",
    "    p = precision_score(y_test, y_pred)\n",
    "    r = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    \n",
    "    tpr = r\n",
    "    fpr = fp / (fp+tn)\n",
    "    fnr = fn / (fn+tp)\n",
    "    tnr = tn / (tn+fp)\n",
    "    \n",
    "    y_score = to_categorical(y_pred)\n",
    "    fpr_, tpr_, _ = roc_curve(y_test, y_score[:, 1])\n",
    "    roc_auc = auc(fpr_, tpr_)\n",
    "\n",
    "    return a, p, r, tpr, fpr, fnr, f1, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bnn_model(neurons, \n",
    "                    input_shape, \n",
    "                    last_act=\"softmax\", \n",
    "                    learning_rate=0.0001, \n",
    "                    loss='squared_hinge'):\n",
    "    \n",
    "    kwargs = dict(input_quantizer=\"ste_sign\",\n",
    "              kernel_quantizer=\"ste_sign\",\n",
    "              kernel_constraint=\"weight_clip\")\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(lq.layers.QuantDense(neurons[0], use_bias=False,\n",
    "                                   input_quantizer=\"ste_sign\",\n",
    "                                   kernel_quantizer=\"ste_sign\",\n",
    "                                   kernel_constraint=\"weight_clip\",\n",
    "                                   input_shape=(input_shape,) ) )\n",
    "    model.add(tf.keras.layers.BatchNormalization(scale=False, momentum=0.9))\n",
    "    model.add(lq.layers.QuantDense(neurons[1], use_bias=False, **kwargs))\n",
    "    model.add(tf.keras.layers.BatchNormalization(scale=False, momentum=0.9))\n",
    "    model.add(lq.layers.QuantDense(neurons[2], use_bias=False, activation=last_act, **kwargs))\n",
    "\n",
    "    # lq.models.summary(model)\n",
    "    \n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=opt, loss=loss, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths_range = [3, 6, 9]\n",
    "estimators_range = [5]\n",
    "\n",
    "bnn_models = [\n",
    "    [32, 16, 2],\n",
    "    [64, 32, 2],\n",
    "    [128, 64, 2]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_folds = 5\n",
    "train_epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy_store_sec = {}\n",
    "precision_store_sec = {}\n",
    "recall_store_sec = {}\n",
    "tpr_store_sec = {}\n",
    "fpr_store_sec = {}\n",
    "fnr_store_sec = {}\n",
    "f1_store_sec = {}\n",
    "roc_auc_store_sec = {}\n",
    "\n",
    "########################################\n",
    "\n",
    "skf = StratifiedShuffleSplit(n_splits=num_folds, random_state=0)\n",
    "\n",
    "for depth in depths_range:    \n",
    "    label = 'dt__depth_%d' % (depth)\n",
    "    accuracy_store_sec[label] = np.zeros(num_folds)\n",
    "    precision_store_sec[label] = np.zeros(num_folds)\n",
    "    recall_store_sec[label] = np.zeros(num_folds)\n",
    "    tpr_store_sec[label] = np.zeros(num_folds)\n",
    "    fpr_store_sec[label] = np.zeros(num_folds)\n",
    "    fnr_store_sec[label] = np.zeros(num_folds)\n",
    "    f1_store_sec[label] = np.zeros(num_folds)\n",
    "    roc_auc_store_sec[label] = np.zeros(num_folds)\n",
    "    \n",
    "    fold_idx = 0\n",
    "    for train_index, test_index in skf.split(X, Y):\n",
    "        print('DT: depth=%d, fold_idx=%d' % (depth, fold_idx))\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = Y__2_classes[train_index], Y__2_classes[test_index]\n",
    "                        \n",
    "        dt = DecisionTreeClassifier(criterion='entropy', max_depth=depth, random_state=0)\n",
    "        dt = dt.fit(X_train, y_train)\n",
    "        y_pred = dt.predict(X_test)\n",
    "        y_score = dt.predict_proba(X_test)\n",
    "\n",
    "        a, p, r, tpr, fpr, fnr, f1, roc_auc = metrics_binary_dataset(dt, X_test, y_test, y_pred, y_score)\n",
    "        accuracy_store_sec[label][fold_idx] = a\n",
    "        precision_store_sec[label][fold_idx] = p\n",
    "        recall_store_sec[label][fold_idx] = r\n",
    "        tpr_store_sec[label][fold_idx] = tpr\n",
    "        fpr_store_sec[label][fold_idx] = fpr\n",
    "        fnr_store_sec[label][fold_idx] = fnr\n",
    "        f1_store_sec[label][fold_idx] = f1\n",
    "        roc_auc_store_sec[label][fold_idx] = roc_auc\n",
    "\n",
    "        fold_idx += 1\n",
    "\n",
    "        print('-'*40)\n",
    "    \n",
    "    print('='*80)\n",
    "\n",
    "########################################################################################\n",
    "\n",
    "for depth in depths_range:\n",
    "    for estimators in estimators_range:\n",
    "        label = 'rf__depth_%d__estimators_%d' % (depth, estimators)\n",
    "        accuracy_store_sec[label] = np.zeros(num_folds)\n",
    "        precision_store_sec[label] = np.zeros(num_folds)\n",
    "        recall_store_sec[label] = np.zeros(num_folds)\n",
    "        tpr_store_sec[label] = np.zeros(num_folds)\n",
    "        fpr_store_sec[label] = np.zeros(num_folds)\n",
    "        fnr_store_sec[label] = np.zeros(num_folds)\n",
    "        f1_store_sec[label] = np.zeros(num_folds)\n",
    "        roc_auc_store_sec[label] = np.zeros(num_folds)\n",
    "\n",
    "        fold_idx = 0\n",
    "        for train_index, test_index in skf.split(X, Y):\n",
    "            print('RF: depth=%d, estimators=%d, fold_idx=%d' % (depth, estimators, fold_idx))\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = Y__2_classes[train_index], Y__2_classes[test_index]\n",
    "\n",
    "            rf = RandomForestClassifier(criterion='entropy', max_depth=depth, n_estimators=estimators, random_state=0)\n",
    "            rf = rf.fit(X_train, y_train)\n",
    "            y_pred = rf.predict(X_test)\n",
    "            y_score = rf.predict_proba(X_test)\n",
    "\n",
    "            a, p, r, tpr, fpr, fnr, f1, roc_auc = metrics_binary_dataset(rf, X_test, y_test, y_pred, y_score)\n",
    "            accuracy_store_sec[label][fold_idx] = a\n",
    "            precision_store_sec[label][fold_idx] = p\n",
    "            recall_store_sec[label][fold_idx] = r\n",
    "            tpr_store_sec[label][fold_idx] = tpr\n",
    "            fpr_store_sec[label][fold_idx] = fpr\n",
    "            fnr_store_sec[label][fold_idx] = fnr\n",
    "            f1_store_sec[label][fold_idx] = f1\n",
    "            roc_auc_store_sec[label][fold_idx] = roc_auc\n",
    "        \n",
    "            fold_idx += 1\n",
    "\n",
    "            print('-'*40)\n",
    "            \n",
    "        print('='*40)\n",
    "\n",
    "    print('='*80)\n",
    "\n",
    "########################################################################################\n",
    "\n",
    "for neurons in bnn_models:\n",
    "    label = 'bnn__%s' % ('_'.join(map(str, neurons)))\n",
    "    accuracy_store_sec[label] = np.zeros(num_folds)\n",
    "    precision_store_sec[label] = np.zeros(num_folds)\n",
    "    recall_store_sec[label] = np.zeros(num_folds)\n",
    "    tpr_store_sec[label] = np.zeros(num_folds)\n",
    "    fpr_store_sec[label] = np.zeros(num_folds)\n",
    "    fnr_store_sec[label] = np.zeros(num_folds)\n",
    "    f1_store_sec[label] = np.zeros(num_folds)\n",
    "    roc_auc_store_sec[label] = np.zeros(num_folds)\n",
    "    \n",
    "    fold_idx = 0\n",
    "    for train_index, test_index in skf.split(X, Y):\n",
    "        print('BNN', neurons ,', fold_idx=%d' % (fold_idx))\n",
    "        X_train, X_test = X_bin[train_index], X_bin[test_index]\n",
    "        y_train, y_test = Y_cat__2_classes[train_index], Y_cat__2_classes[test_index]\n",
    "                        \n",
    "        model = build_bnn_model(neurons, X_bin.shape[1])   \n",
    "        fname = 'bnn__sec_StratifiedShuffleSplit__%s__fold%d.h5' % ('_'.join(map(str, neurons)), fold_idx)\n",
    "        \n",
    "        model_checkpoint_callback = ModelCheckpoint(\n",
    "            filepath='models/' + fname,\n",
    "            monitor='val_accuracy',\n",
    "            mode='max',\n",
    "            save_best_only=True,\n",
    "            verbose=1)\n",
    "        \n",
    "        if not os.path.isfile('models/' + fname):\n",
    "            train_history = model.fit(X_train, y_train, \n",
    "                              batch_size=batch_size, \n",
    "                              epochs=train_epochs,\n",
    "                              verbose=1,\n",
    "                              validation_data=(X_test, y_test),\n",
    "                              callbacks=[model_checkpoint_callback])\n",
    "                    \n",
    "            # Reload best weights\n",
    "            model.load_weights('models/' + fname)\n",
    "        else:\n",
    "            # Reload stored weights\n",
    "            print('Loading models/' + fname)\n",
    "            model.load_weights('models/' + fname)\n",
    "\n",
    "        y_pred = model.predict_classes(X_test)\n",
    "        y_score = model.predict_proba(X_test)\n",
    "        \n",
    "        a, p, r, tpr, fpr, fnr, f1, roc_auc = metrics_binary_dataset(model, X_test, y_test, y_pred, y_score, is_bnn=True)\n",
    "        accuracy_store_sec[label][fold_idx] = a\n",
    "        precision_store_sec[label][fold_idx] = p\n",
    "        recall_store_sec[label][fold_idx] = r\n",
    "        tpr_store_sec[label][fold_idx] = tpr\n",
    "        fpr_store_sec[label][fold_idx] = fpr\n",
    "        fnr_store_sec[label][fold_idx] = fnr\n",
    "        f1_store_sec[label][fold_idx] = f1\n",
    "        roc_auc_store_sec[label][fold_idx] = roc_auc\n",
    "        \n",
    "        fold_idx += 1\n",
    "        \n",
    "        print('-'*40)\n",
    "    \n",
    "    print('='*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for store,metric in zip([accuracy_store_sec, precision_store_sec, recall_store_sec,\n",
    "                         fnr_store_sec, fpr_store_sec, f1_store_sec, roc_auc_store_sec],\n",
    "                        ['Accuracy', 'Precision', 'Recall', 'FNR', 'FPR', 'F1-score', 'ROC-AUC']):\n",
    "    print('[%s]' % metric)\n",
    "    for key in store:\n",
    "        print('%s: %.1f ± %.1f' % (key, 100*store[key].mean(), 100*store[key].std()))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
